library(effects)
library(epitools)
library(foreign)
library(ggplot2)
library(MASS)
library(mlbench)
library(nnet)
library(Rcmdr)
library(RcmdrMisc)
library(sandwich)
library(reshape2)
library(xtable)
library(fpc)
library(dbscan)
library(cluster)
library(dendextend)
library(devtools)
library(factoextra)
library(rpart.plot)
library(tree)
library(cvTools)
library(boot)
library(DAAG)
#Reading dataset
dataset <- read.csv("kc_house_data.csv")
houseData <- subset(dataset, select = c("price","bedrooms","bathrooms","sqft_living","sqft_lot","floors","waterfront","view","condition","grade","sqft_above","sqft_basement","yr_built","yr_renovated","zipcode","lat","long","sqft_living15","sqft_lot15"))
# Point number 1
summary(houseData$price)
sd(houseData$price)
# Point number 2
# Partitioning dataset in 80% training data and 20% testing data.
# 80% of the sample size
smp_size <- floor(0.80 * nrow(houseData))
## set the seed to make your partition reproductible
set.seed(20161101)
train_sample <- sample(seq_len(nrow(houseData)), size = smp_size)
#Obtaining training and test portions from house data
house.Train <- houseData[train_sample, ]
house.Test <- houseData[-train_sample, ]
# 2.a cases in training data set
nrow(house.Train)
# 2.b cases in test data set
nrow(house.Test)
# 2.c
# Summary for Training set
summary(house.Train$price)
sd(house.Train$price)
# Summary for Test set
summary(house.Test$price)
sd(house.Test$price)
# 2.d
# Fitting a multiple linear regression model
hPrice.Tr.lm <- lm(price~., data=house.Train)
summary(hPrice.Tr.lm)
#anova(house.Train.lm)
# 2.e
# Use the automatic forward/backward selection method to derive the \best" model
hPrice.null <- lm(price~ 1, data=house.Train)
housing.best <- stepAIC(hPrice.null, scope=list(upper=hPrice.Tr.lm,lower=hPrice.null),direction="both")
summary(housing.best)
# 2.f
# Draw scatterplots of the residuals in the \best" model against each of the predictors.
par(mfrow=c(3,3))
crPlots(housing.best, line = TRUE, smooth = TRUE)
rm(list = ls())
#Reading dataset
dataset <- read.csv("kc_house_data.csv")
houseData <- subset(dataset, select = c("price","bedrooms","bathrooms","sqft_living","sqft_lot","floors","waterfront","view","condition","grade","sqft_above","sqft_basement","yr_built","yr_renovated","zipcode","lat","long","sqft_living15","sqft_lot15"))
# Point number 1
summary(houseData$price)
sd(houseData$price)
# Point number 2
# Partitioning dataset in 80% training data and 20% testing data.
# 80% of the sample size
smp_size <- floor(0.80 * nrow(houseData))
## set the seed to make your partition reproductible
set.seed(20161101)
train_sample <- sample(seq_len(nrow(houseData)), size = smp_size)
#Obtaining training and test portions from house data
house.Train <- houseData[train_sample, ]
house.Test <- houseData[-train_sample, ]
# 2.a cases in training data set
nrow(house.Train)
# 2.b cases in test data set
nrow(house.Test)
# 2.c
# Summary for Training set
summary(house.Train$price)
sd(house.Train$price)
# Summary for Test set
summary(house.Test$price)
sd(house.Test$price)
# 2.d
# Fitting a multiple linear regression model
hPrice.Tr.lm <- lm(price~., data=house.Train)
summary(hPrice.Tr.lm)
#anova(house.Train.lm)
# 2.e
# Use the automatic forward/backward selection method to derive the \best" model
hPrice.null <- lm(price~ 1, data=house.Train)
housing.best <- stepAIC(hPrice.null, scope=list(upper=hPrice.Tr.lm,lower=hPrice.null),direction="both")
summary(housing.best)
# 2.f
# Draw scatterplots of the residuals in the \best" model against each of the predictors.
par(mfrow=c(3,3))
crPlots(housing.best, line = TRUE, smooth = TRUE)
# 2.g
#---------------------------------------------------
# Selected predictors with non-linear effects
# sqft_living, bathrooms, sqft_above
#---------------------------------------------------
#Calculate time for CV task to put a delay and wait for this task to complete (in nested for loops).
start.time <- Sys.time()
lm.Highest.Ply <- lm(price ~. - floors - sqft_basement + poly(sqft_living, 5) + poly(bathrooms, 5) + poly(sqft_above, 5), data=houseData)
cv.Highest.Ply <- cv.lm(houseData, lm.Highest.Ply, m = 5, plotit = FALSE)
MSE.p <- attr(cv.Highest.Ply, "ms")
end.time <- Sys.time()
taken.time <- (end.time - start.time) %>% round()
#taken.time
#vector to save the MSE of each CV for each Polynomial
MSE.Ply <- rep(0, 125)
folds <- 5
#---------------------------------------------------
# For loop to perform iteratively cross validation and linear models with polynomials for
# "sqft_living","bathrooms" and "sqft_above".
for(i in 1:5){
for(j in 1:5){
for(k in 1:5){
#Linear model for all the polynomials of "sqft_living", "bathrooms", "sqft_above"
lm.Poly <- lm(price ~. - floors - sqft_basement + poly(sqft_living, i) + poly(bathrooms, j) + poly(sqft_above, k), data=houseData)
cv.Poly <- cv.lm(houseData, lm.Poly, m = folds, plotit = FALSE)
#Save MSE of each polynomial for the three selected predictors for ploting (in order)
MSE.Ply[((i-1)*25) + ((j-1)*5) + k] <- attr(cv.Poly,"ms")
#Delay in system to allow R finishing cross validation before continuing with the next
Sys.sleep(taken.time)
}
}
}
#Obtain index of minimum MSE
which.min(MSE.Ply)
#Get MSE of the CV with the minimum value
MSE.min <- MSE.Ply[which.min(MSE.Ply)]
#Print Cross-Validation results of the polynomial with the minimum MSE
lm.min.MSE <- lm(price ~. - floors - sqft_basement + poly(sqft_living, 2) + poly(bathrooms, 2) + poly(sqft_above, 1), data=houseData)
cv.min.MSE <- cv.lm(houseData, lm.Highest.Ply, m = 5, plotit = FALSE)
#Vector with MSEs of each fold for the minimum MSE of the polynomials
MSEk.31 <- c(4.27e+10, 3.8e+10, 3.41e+10, 3.93e+10, 3.27e+10)
which.min(MSE.Ply)
MSE.min <- MSE.Ply[which.min(MSE.Ply)]
lm.min.MSE <- lm(price ~. - floors - sqft_basement + poly(sqft_living, 2) + poly(bathrooms, 2) + poly(sqft_above, 1), data=houseData)
cv.min.MSE <- cv.lm(houseData, lm.Highest.Ply, m = 5, plotit = FALSE)
MSEk.31 <- c(4.27e+10, 3.8e+10, 3.41e+10, 3.93e+10, 3.27e+10)
SE.MSEk.31 <- sd(MSEk.31)/sqrt(folds)
SE.MSEk.31
Poly.list <- (1:125)
par(mfrow=c(1,1))
plot(Poly.list,MSE.Ply, type = "o", col = "chocolate1", lwd = 2, main = "5-fold CV for all possible polynomials", xlab = "Number of Polynomial", ylab = "Mean Squared Error")
abline(as.numeric(MSE.min + SE.MSEk.31), b = 0,col = "red", lwd = 2)
SE.MSEk.31
check <- rep(NA , 20000)
for (i in 1:20000) {
check[i] <- sum(sample(1:1000), rep = TRUE) == 7 > 0
}
for (i in 1:20000) {
check[i] <- sum(sample(1:1000), rep = TRUE) == 7 > 0
}
?sum
#View(MSE.Ply.sqft_above)
#Plot of the MSE for "sqft_above"
plot(order,MSE.Ply.sqft_above, type = "o",  col = "forestgreen", lwd = 2, main = "5-fold CV - sqft_above", xlab = "Degree of Polynomial", ylab = "Mean Squared Error")
knitr::opts_chunk$set(echo = TRUE)
summary(cars)
dataset <- read.csv("kc_house_data.csv")
houseData <- subset(dataset, select = c("price","bedrooms","bathrooms","sqft_living","sqft_lot","floors","waterfront","view","condition","grade","sqft_above","sqft_basement","yr_built","yr_renovated","zipcode","lat","long","sqft_living15","sqft_lot15"))
dataset <- read.csv("kc_house_data.csv")
houseData <- subset(dataset, select = c("price","bedrooms","bathrooms","sqft_living","sqft_lot","floors","waterfront","view","condition","grade","sqft_above","sqft_basement","yr_built","yr_renovated","zipcode","lat","long","sqft_living15","sqft_lot15"))
summary(houseData$price)
sd(houseData$price)
smp_size <- floor(0.80 * nrow(houseData))
set.seed(20161101)
train_sample <- sample(seq_len(nrow(houseData)), size = smp_size)
house.Train <- houseData[train_sample, ]
house.Test <- houseData[-train_sample, ]
dim(houseData)
dim(houseData)
dim(house.Train)
dim(house.Test)
# Summary for Training set
summary(house.Train$price)
sd(house.Train$price)
# Summary for Test set
summary(house.Test$price)
sd(house.Test$price)
hPrice.Tr.lm <- lm(price~., data=house.Train)
summary(hPrice.Tr.lm)
hPrice.null <- lm(price~ 1, data=house.Train)
housing.best <- stepAIC(hPrice.null, scope=list(upper=hPrice.Tr.lm,lower=hPrice.null),direction="both")
library(MASS)
hPrice.null <- lm(price~ 1, data=house.Train)
housing.best <- stepAIC(hPrice.null, scope=list(upper=hPrice.Tr.lm,lower=hPrice.null),direction="both")
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
hPrice.null <- lm(price~ 1, data=house.Train)
housing.best <- stepAIC(hPrice.null, scope=list(upper=hPrice.Tr.lm,lower=hPrice.null),direction="both")
echo?
?echo
hPrice.null <- lm(price~ 1, data=house.Train)
housing.best <- stepAIC(hPrice.null, scope=list(upper=hPrice.Tr.lm,lower=hPrice.null),direction="both")
hPrice.null <- lm(price~ 1, data=house.Train)
housing.best <- stepAIC(hPrice.null, scope=list(upper=hPrice.Tr.lm,lower=hPrice.null),direction="both")
hPrice.null <- lm(price~ 1, data=house.Train)
housing.best <- stepAIC(hPrice.null, scope=list(upper=hPrice.Tr.lm,lower=hPrice.null),direction="both")
summary(housing.best)
par(mfrow=c(3,3))
crPlots(housing.best, line = TRUE, smooth = TRUE)
library(car)
install.packages("car")
library(MASS)
library(car)
par(mfrow=c(3,3))
crPlots(housing.best, line = TRUE, smooth = TRUE)
par(mfrow=c(4, 4))
crPlots(housing.best, line = TRUE, smooth = TRUE)
par(mfrow=c(4, 4))
crPlots(housing.best, line = TRUE, smooth = TRUE)
par(mfrow=c(6, 3))
crPlots(housing.best, line = TRUE, smooth = TRUE)
par(mfrow=c(3, 6))
crPlots(housing.best, line = TRUE, smooth = TRUE)
View(house.Test)
house.Test[4,1]
#vector to save the MSE of each CV for each Polynomial
MSE.Ply <- rep(0, 125)
folds <- 5
for(i in 1:5){
for(j in 1:5){
for(k in 1:5){
#Linear model for all the polynomials of "sqft_living", "bathrooms", "sqft_above"
lm.Poly <- lm(price ~. - floors - sqft_basement + poly(sqft_living, i) + poly(bathrooms, j) + poly(sqft_above, k), data=houseData)
cv.Poly <- cv.lm(houseData, lm.Poly, m = folds, plotit = FALSE)
#Save MSE of each polynomial for the three selected predictors for ploting (in order)
MSE.Ply[((i-1)*25) + ((j-1)*5) + k] <- attr(cv.Poly,"ms")
#Delay in system to allow R finishing cross validation before continuing with the next
Sys.sleep(taken.time)
}
}
}
library(MASS)
library(car)
library(DAAG)
#vector to save the MSE of each CV for each Polynomial
MSE.Ply <- rep(0, 125)
folds <- 5
for(i in 1:5){
for(j in 1:5){
for(k in 1:5){
#Linear model for all the polynomials of "sqft_living", "bathrooms", "sqft_above"
lm.Poly <- lm(price ~. - floors - sqft_basement + poly(sqft_living, i) + poly(bathrooms, j) + poly(sqft_above, k), data=houseData)
cv.Poly <- cv.lm(houseData, lm.Poly, m = folds, plotit = FALSE)
#Save MSE of each polynomial for the three selected predictors for ploting (in order)
MSE.Ply[((i-1)*25) + ((j-1)*5) + k] <- attr(cv.Poly,"ms")
#Delay in system to allow R finishing cross validation before continuing with the next
Sys.sleep(taken.time)
}
}
}
install.packages("DAAG")
library(MASS)
library(car)
library(DAAG)
#vector to save the MSE of each CV for each Polynomial
MSE.Ply <- rep(0, 125)
folds <- 5
for(i in 1:5){
for(j in 1:5){
for(k in 1:5){
#Linear model for all the polynomials of "sqft_living", "bathrooms", "sqft_above"
lm.Poly <- lm(price ~. - floors - sqft_basement + poly(sqft_living, i) + poly(bathrooms, j) + poly(sqft_above, k), data=houseData)
cv.Poly <- cv.lm(houseData, lm.Poly, m = folds, plotit = FALSE)
#Save MSE of each polynomial for the three selected predictors for ploting (in order)
MSE.Ply[((i-1)*25) + ((j-1)*5) + k] <- attr(cv.Poly,"ms")
#Delay in system to allow R finishing cross validation before continuing with the next
Sys.sleep(taken.time)
}
}
}
Poly.list
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(car)
library(DAAG)
#vector to save the MSE of each CV for each Polynomial
MSE.Ply <- rep(0, 125)
folds <- 5
for(i in 1:5){
for(j in 1:5){
for(k in 1:5){
#Linear model for all the polynomials of "sqft_living", "bathrooms", "sqft_above"
lm.Poly <- lm(price ~. - floors - sqft_basement + poly(sqft_living, i) + poly(bathrooms, j) + poly(sqft_above, k), data=houseData)
cv.Poly <- cv.lm(houseData, lm.Poly, m = folds, plotit = FALSE)
#Save MSE of each polynomial for the three selected predictors for ploting (in order)
MSE.Ply[((i-1)*25) + ((j-1)*5) + k] <- attr(cv.Poly,"ms")
#Delay in system to allow R finishing cross validation before continuing with the next
Sys.sleep(taken.time)
}
}
}
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(car)
library(DAAG)
start.time <- Sys.time()
lm.Highest.Ply <- lm(price ~. - floors - sqft_basement + poly(sqft_living, 5) + poly(bathrooms, 5) + poly(sqft_above, 5), data=houseData)
cv.Highest.Ply <- cv.lm(houseData, lm.Highest.Ply, m = 5, plotit = FALSE)
MSE.p <- attr(cv.Highest.Ply, "ms")
end.time <- Sys.time()
taken.time <- (end.time - start.time) %>% round()
library(MASS)
library(dplyr)
start.time <- Sys.time()
lm.Highest.Ply <- lm(price ~. - floors - sqft_basement + poly(sqft_living, 5) + poly(bathrooms, 5) + poly(sqft_above, 5), data=houseData)
cv.Highest.Ply <- cv.lm(houseData, lm.Highest.Ply, m = 5, plotit = FALSE)
MSE.p <- attr(cv.Highest.Ply, "ms")
end.time <- Sys.time()
taken.time <- (end.time - start.time) %>% round()
taken.time
?Sys.sleep
#vector to save the MSE of each CV for each Polynomial
MSE.Ply <- rep(0, 125)
folds <- 5
for(i in 1:5){
for(j in 1:5){
for(k in 1:5){
#Linear model for all the polynomials of "sqft_living", "bathrooms", "sqft_above"
lm.Poly <- lm(price ~. - floors - sqft_basement + poly(sqft_living, i) + poly(bathrooms, j) + poly(sqft_above, k), data=houseData)
cv.Poly <- cv.lm(houseData, lm.Poly, m = folds, plotit = FALSE)
#Save MSE of each polynomial for the three selected predictors for ploting (in order)
MSE.Ply[((i-1)*25) + ((j-1)*5) + k] <- attr(cv.Poly,"ms")
#Delay in system to allow R finishing cross validation before continuing with the next
Sys.sleep(8)
}
}
}
#Obtain index of minimum MSE
which.min(MSE.Ply)
#Get MSE of the CV with the minimum value
MSE.min <- MSE.Ply[which.min(MSE.Ply)]
#Get MSE of the CV with the minimum value
MSE.min <- MSE.Ply[which.min(MSE.Ply)]
MSE.min
#Get MSE of the CV with the minimum value
MSE.min <- MSE.Ply[which.min(MSE.Ply)]
MSE.min
#Get MSE of the CV with the minimum value
MSE.min <- MSE.Ply[which.min(MSE.Ply)]
MSE.min
lm.min.MSE <- lm(price ~. - floors - sqft_basement + poly(sqft_living, 2) + poly(bathrooms, 2) + poly(sqft_above, 1), data=houseData)
cv.min.MSE <- cv.lm(houseData, lm.Highest.Ply, m = 5, plotit = FALSE)
lm.min.MSE <- lm(price ~. - floors - sqft_basement + poly(sqft_living, 2) + poly(bathrooms, 2) + poly(sqft_above, 1), data=houseData)
cv.min.MSE <- cv.lm(houseData, lm.Highest.Ply, m = 5, plotit = FALSE)
# Vector with MSEs of each fold for the minimum MSE of the polynomials
MSEk.31 <- c(5.44e+10, 3.79e+10, 3.32e+10, 4.46e+10, 3.32e+10)
# Calculating standard error
SE.MSEk.31 <- sd(MSEk.31)/sqrt(folds)
# Print MSE value
SE.MSEk.31
# Plot crossvalidations' MSE and the SD from the lowest MSE
Poly.list <- (1:125)
par(mfrow=c(1,1))
plot(Poly.list,MSE.Ply, type = "o", col = "chocolate1", lwd = 2, main= "5-fold CV for all possible polynomials", xlab = "Polynomial Index", ylab = "Mean Squared Error")
abline(as.numeric(MSE.min + SE.MSEk.31), b = 0,col = "red", lwd = 2)
# Vector with MSEs of each fold for the minimum MSE of the polynomials
MSEk.31 <- c(4.27e+10, 3.8e+10, 3.41e+10, 3.93e+10, 3.27e+10)
# Calculating standard error
SE.MSEk.31 <- sd(MSEk.31)/sqrt(folds)
# Print MSE value
SE.MSEk.31
# Plot crossvalidations' MSE and the SD from the lowest MSE
Poly.list <- (1:125)
par(mfrow=c(1,1))
plot(Poly.list,MSE.Ply, type = "o", col = "chocolate1", lwd = 2, main= "5-fold CV for all possible polynomials", xlab = "Polynomial Index", ylab = "Mean Squared Error")
abline(as.numeric(MSE.min + SE.MSEk.31), b = 0,col = "red", lwd = 2)
lm.simple <- lm(price ~. - floors - sqft_basement + sqft_living + bathrooms + poly(sqft_above, 2), data=house.Train)
cv.simple <- cv.lm(house.Train, lm.Highest.Ply, m = 5, plotit = FALSE)
lm.simple <- lm(price ~. - floors - sqft_basement + sqft_living + bathrooms + poly(sqft_above, 2), data=house.Train)
cv.simple <- cv.lm(house.Train, lm.simple, m = 5, plotit = FALSE)
?predict
predictions <- predict(cv.simple, house.Test)
pred.w.plim <- predict(cv.simple, house.Test, interval = "prediction")
predictions <- predict(lm.simple, house.Test)
pred.w.plim <- predict(lm.simple, house.Test, interval = "prediction")
predictions <- predict(cv.simple, house.Test)
pred.w.plim <- predict(cv.simple, house.Test, interval = "prediction")
View(cv.simple)
View(cv.simple)
View(lm.simple)
library(MASS)
library(car)
library(DAAG)
library(caret)
train.control <- trainControl(method = "cv", number = 5)
library(MASS)
library(car)
library(DAAG)
library(caret)
install.packages("caret")
install.packages("caret")
library(MASS)
library(car)
library(DAAG)
library(caret)
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(car)
install.packages("rlang")
library(MASS)
library(car)
library(DAAG)
library(caret)
train.control <- trainControl(method = "cv", number = 5)
# Train the model
cv.simple <- train(price ~. - floors - sqft_basement + sqft_living + bathrooms + poly(sqft_above, 2), data=house.Train, method = "lm", trControl = train.control)
testPrediction <- predict(cv.simple, house.Test, interval = "prediction")
View(testPrediction)
testPrediction <- predict(cv.simple, house.Test, interval = "prediction")
train.control <- trainControl(method = "cv", number = 5)
# Train the model
cv.simple <- train(price ~. - floors - sqft_basement + sqft_living + bathrooms + poly(sqft_above, 2), data=house.Train, method = "lm", trControl = train.control)
testPrediction <- predict(cv.simple, house.Test)
testPrediction[1:50]
house.Test$price[1:50]
install.packages("MLmetrics")
library(MASS)
library(car)
library(DAAG)
library(caret)
library(MLmetrics)
MSE(testPrediction, house.Test$price)
install.packages("dvmisc")
library(MASS)
library(car)
library(DAAG)
library(caret)
library(dvmisc)
testPrediction <- predict(cv.simple, interval= "prediction")
train.control <- trainControl(method = "cv", number = 5)
# Train the model
cv.simple <- train(price ~. - floors - sqft_basement + sqft_living + bathrooms + poly(sqft_above, 2), data=house.Train, method = "lm", trControl = train.control)
testPrediction <- predict(cv.simple, interval= "prediction")
train.control <- trainControl(method = "cv", number = 5)
# Train the model
cv.simple <- train(price ~. - floors - sqft_basement + sqft_living + bathrooms + sqft_above^2, data=house.Train, method = "lm", trControl = train.control)
testPrediction <- predict(cv.simple, interval= "prediction")
View(testPrediction)
testPrediction <- predict(cv.simple, house.Test, interval= "prediction")
View(testPrediction)
testPrediction[1:10]
View(house.Test)
rownames(house.Test) <- NULL
house.Test[1:10]
testPrediction[1:10]
house.Test$price[1:10]
library(MASS)
library(car)
library(DAAG)
library(caret)
library(MLmetrics)
testPrediction <- predict(cv.simple, house.Test)
RMSE(testPrediction, house.Test$price)
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(car)
library(DAAG)
library(caret)
library(MLmetrics)
library(MASS)
library(car)
library(DAAG)
library(caret)
library(MLmetrics)
library(ggplot2)
# Add predictions
pred.int <- predict(cv.simple, interval = "prediction")
mydata <- cbind(house.Test, pred.int)
# Add predictions
pred.int <- predict(object = cv.simple, newdata = house.Test, interval = "prediction")
mydata <- cbind(house.Test, pred.int)
p <- ggplot(mydata, aes(grade, price)) +
geom_point() +
stat_smooth(method = lm)
# Add prediction intervals
p + geom_line(aes(y = lwr), color = "red", linetype = "dashed")+
geom_line(aes(y = upr), color = "red", linetype = "dashed")
pred.int
testPrediction <- predict(cv.simple, house.Test)
RMSE(testPrediction, house.Test$price)
summary(cv.simple)
testPrediction <- predict(cv.simple, house.Test)
RMSE(testPrediction, house.Test$price)
summary(cv.simple)
testPrediction <- predict(cv.simple, house.Test)
print("Root Mean Squared Error")
RMSE(testPrediction, house.Test$price)
summary(cv.simple)
testPrediction <- predict(cv.simple, house.Test)
print(Root Mean Squared Error)
testPrediction <- predict(cv.simple, house.Test)
print("Root Mean Squared Error")
RMSE(testPrediction, house.Test$price)
summary(cv.simple)
